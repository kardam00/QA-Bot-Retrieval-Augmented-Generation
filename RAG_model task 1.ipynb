{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WuytpPE29U6y",
        "IEbKMkY8HZJy",
        "bzELhE8iVCHS",
        "es1Zk1t1VEFH",
        "m04ZYx8MWrii",
        "v4JlBRmz9FtF",
        "4kNdToB8ohAG",
        "usnlDCrRo2Kz",
        "TTdQFAjSo6X3",
        "AsYrfYdUo_p5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Retrieval-Augmented Generation (RAG) Model for QA Bot\n"
      ],
      "metadata": {
        "id": "WuytpPE29U6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up the Environment"
      ],
      "metadata": {
        "id": "IEbKMkY8HZJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Packages:\n",
        "\n",
        "```pinecone-client``` for Pinecone DB.\n",
        "\n",
        "*   ```transformers``` for handling embeddings.\n",
        "*   ```datasets``` if you plan to use a dataset for testing.\n",
        "*   ```Cohere API``` for the generative model."
      ],
      "metadata": {
        "id": "68JBIcH3_Fqh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biBNup5z_Ex6"
      },
      "outputs": [],
      "source": [
        "!pip install pinecone-client transformers datasets cohere\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Up Pinecone:"
      ],
      "metadata": {
        "id": "bzELhE8iVCHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(\n",
        "    api_key=\"b8fa39d7-0b89-4e9c-9534-81216e350b0b\"\n",
        ")\n",
        "\n",
        "index_name = \"qa-bot-index\"\n",
        "# Connect to the index\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "KYQNy_Rc_HUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Up Cohere API:"
      ],
      "metadata": {
        "id": "es1Zk1t1VEFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "\n",
        "# Initialize Cohere client\n",
        "co = cohere.Client(\"4Xp1G1vwLELRyqkySME5Px88rW1JkiEsMfEC28RJ\")\n"
      ],
      "metadata": {
        "id": "dXwHs3Lt_HRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the SQuAD Dataset\n"
      ],
      "metadata": {
        "id": "m04ZYx8MWrii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# Load the SQuAD (Stanford Question Answering Dataset)\n",
        "squad_dataset = load_dataset(\"squad\")\n",
        "print(squad_dataset['train'])\n",
        "\n",
        "contexts = [entry['context'] for entry in squad_dataset['train']]\n",
        "questions = [entry['question'] for entry in squad_dataset['train']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de-TzrJR_HOi",
        "outputId": "127af682-e784-4ef5-85fa-9b1636742350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
            "    num_rows: 87599\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking Contexts"
      ],
      "metadata": {
        "id": "v4JlBRmz9FtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to split contexts into smaller chunks\n",
        "def split_into_chunks(context, max_length=100):\n",
        "    sentences = sent_tokenize(context)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) <= max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Split all contexts into smaller chunks\n",
        "document_chunks = []\n",
        "for context in contexts:\n",
        "    chunks = split_into_chunks(context)\n",
        "    document_chunks.extend(chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3NtSnjkoalZ",
        "outputId": "12a85887-a317-4a46-f7db-67fc1994f07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Embeddings\n"
      ],
      "metadata": {
        "id": "4kNdToB8ohAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load Sentence-BERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Generate embeddings in batches\n",
        "def generate_embeddings_batch(texts, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {key: val.cuda() for key, val in inputs.items()}\n",
        "            model.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "    return torch.cat([torch.tensor(batch) for batch in embeddings])\n",
        "\n",
        "# Generate embeddings for document chunks\n",
        "batch_size = 16\n",
        "doc_embeddings = generate_embeddings_batch(document_chunks, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3Luj-jOomrO",
        "outputId": "1eaccfe9-9157-48ec-becf-b11b99754d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            " 14%|█▎        | 4326/31876 [31:03<3:37:33,  2.11it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying Pinecone"
      ],
      "metadata": {
        "id": "usnlDCrRo2Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate query embedding\n",
        "def generate_query_embedding(query):\n",
        "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {key: val.cuda() for key, val in inputs.items()}\n",
        "        model.cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    query_embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "    return query_embedding.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "mZibCErIpPe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Pinecone with the query embedding\n",
        "def query_pinecone(query, top_k=5):\n",
        "    query_embedding = generate_query_embedding(query)\n",
        "\n",
        "    results = index.query(vector=query_embedding.tolist(), top_k=top_k)\n",
        "\n",
        "    document_ids = [match['id'] for match in results['matches']]\n",
        "    return document_ids"
      ],
      "metadata": {
        "id": "N2jnYGdB45_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the Final Answer"
      ],
      "metadata": {
        "id": "TTdQFAjSo6X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answer based on relevant chunks and user query\n",
        "def generate_answer(contexts, question):\n",
        "    context_text = \" \".join(contexts)\n",
        "\n",
        "    response = co.generate(\n",
        "        model=\"command\",\n",
        "        prompt=f\"Context: {context_text}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
        "        max_tokens=150,\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    return response.generations[0].text.strip()"
      ],
      "metadata": {
        "id": "5Kq3SpIFpTn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve answer based on query\n",
        "def get_answer_from_query(query, top_k=5):\n",
        "    relevant_chunk_ids = query_pinecone(query, top_k=top_k)\n",
        "\n",
        "    relevant_chunks = [document_chunks[int(id)] for id in relevant_chunk_ids]  # Assuming IDs match the chunk indices\n",
        "\n",
        "    final_answer = generate_answer(relevant_chunks, query)\n",
        "\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "7eDjW44l49_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the QA Bot"
      ],
      "metadata": {
        "id": "AsYrfYdUo_p5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What is the role of AI in healthcare?\"\n",
        "answer = get_answer_from_query(user_query, top_k=5)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "2REbBP2YnPt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"How does machine learning improve customer experience?\"\n",
        "answer = get_answer_from_query(user_query, top_k=5)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "eh2sNvhknPra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What are the key components of a neural network?\"\n",
        "answer = get_answer_from_query(user_query, top_k=5)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "Dxcn6fyK6yp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"How is natural language processing (NLP) used in sentiment analysis?\"\n",
        "answer = get_answer_from_query(user_query, top_k=5)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "CuRykEgp6ylW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What are the advantages of edge computing in IoT?\"\n",
        "answer = get_answer_from_query(user_query, top_k=5)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "JXvACq5R6yi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHENrnnY7oK9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}